{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this notebook to scrape votes of the Sight and Sound 2012 Poll. \n",
    "\n",
    "All data are stored in JSON files. The films are saved in FILMS_PATH as an array of film objects of name, year, director, country and url. \n",
    "\n",
    "The voters are saved in VOTERS_PATH as an array of voter objects of name, role, country, gender and url. \n",
    "\n",
    "The votes are saved in VOTES_PATH as an array of pairs of voter index and film index. Each pair denotes a vote.\n",
    "\n",
    "The comments are saved in COMMENTS_PATH as an array of strings, in the same order as the voters. If a voter does not provide a comment, an empty string is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os.path\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILMS_URL = 'https://www.bfi.org.uk/films-tv-people/sightandsoundpoll2012/films'\n",
    "VOTEFS_URL ='https://www.bfi.org.uk/films-tv-people/sightandsoundpoll2012/voters'\n",
    "\n",
    "# List of films is saved in FILMS_PATH\n",
    "# List of voters is saved in VOTERS_PATH\n",
    "# List of voter-film pair is saved in VOTES_PATH \n",
    "# List of voters' comments is saved in COMMENTS_PATH\n",
    "FOLDER_PATH = 'data'\n",
    "FILMS_PATH = '{}/films.json'.format(FOLDER_PATH)\n",
    "VOTERS_PATH = '{}/voters.json'.format(FOLDER_PATH)\n",
    "VOTES_PATH = '{}/votes.json'.format(FOLDER_PATH)\n",
    "COMMENTS_PATH = '{}/comments.json'.format(FOLDER_PATH)\n",
    "SCRAPE_DELAY = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_films():\n",
    "    if os.path.exists(FILMS_PATH):\n",
    "        print('Films are parsed already')\n",
    "        return\n",
    "    films_page = requests.get(FILMS_URL)\n",
    "    soup = BeautifulSoup(films_page.content, 'html.parser')\n",
    "    film_soup = soup.find_all('tr')\n",
    "    films = []\n",
    "    for fs in film_soup:\n",
    "        f_list = fs.get_text().split('\\n')\n",
    "        year_re = re.findall(r'(?<=\\()\\d{4}(?=\\)$)', f_list[1])\n",
    "        year = None if len(year_re) == 0 else int(year_re[0])\n",
    "        name = f_list[1] if len(year_re) == 0 else f_list[1][0:len(f_list[1]) - 7]\n",
    "        url_soup = fs.find('a', href=True)\n",
    "        url = None if url_soup is None else url_soup['href']\n",
    "        films.append({\n",
    "            'name': name,\n",
    "            'year': year,\n",
    "            'director': f_list[2],\n",
    "            'country': f_list[3],\n",
    "            'url': url,\n",
    "        })\n",
    "    with open(FILMS_PATH, 'w') as outfile:\n",
    "        json.dump(films, outfile)\n",
    "    print('Scraped films')\n",
    "    print('Saved films in {}'.format(FILMS_PATH))\n",
    "    time.sleep(SCRAPE_DELAY)\n",
    "\n",
    "def scrape_voters():\n",
    "    if os.path.exists(FILMS_PATH):\n",
    "        print('Films are parsed already')\n",
    "        return\n",
    "    voters_page = requests.get(VOTEFS_URL)\n",
    "    soup = BeautifulSoup(voters_page.content, 'html.parser')\n",
    "    voter_soup = soup.find_all('tr')\n",
    "    voters = []\n",
    "    for vs in voter_soup:\n",
    "        v_list = vs.get_text().split('\\n')\n",
    "        voters.append({\n",
    "            'name': v_list[1],\n",
    "            'role': v_list[2].lower(),\n",
    "            'country': v_list[3],\n",
    "            'gender': v_list[4].lower(),\n",
    "            'url': vs.find('a', href=True)['href'],\n",
    "        })\n",
    "    with open(VOTERS_PATH, 'w') as outfile:\n",
    "        json.dump(voters, outfile)\n",
    "    print('Scraped voters')\n",
    "    print('Saved voters in {}'.format(VOTERS_PATH))\n",
    "    time.sleep(SCRAPE_DELAY)\n",
    "\n",
    "def scrape_votes(films, voters):\n",
    "    print('Votes are saved in {}'.format(VOTES_PATH))\n",
    "    print('Comments are saved in {}'.format(COMMENTS_PATH))\n",
    "    # Represent votes with a list of row-col index pairs\n",
    "    if os.path.exists(VOTES_PATH):\n",
    "        with open(VOTES_PATH) as json_file:\n",
    "            votes_data = json.load(json_file)\n",
    "    else:\n",
    "        votes_data = []\n",
    "    if os.path.exists(COMMENTS_PATH):\n",
    "        with open(COMMENTS_PATH) as json_file:\n",
    "            comments = json.load(json_file)\n",
    "    else:\n",
    "        comments = []\n",
    "    row_idx_start = 0 if len(votes_data) == 0 else votes_data[-1][0] + 1\n",
    "    if len(comments) != row_idx_start:\n",
    "        raise RuntimeError('votes data and comments have different lengths')\n",
    "    print('Start scraping from voter id {}'.format(row_idx_start))\n",
    "    # Scrape votes and append to stored data\n",
    "    for row_idx in range(row_idx_start, len(voters)):\n",
    "        voter = voters[row_idx]\n",
    "        vote_page = requests.get(voter['url'])\n",
    "        soup = BeautifulSoup(vote_page.content, 'html.parser')\n",
    "        vote_soup = soup.find_all('tr')\n",
    "        if len(vote_soup) != 10:\n",
    "            print(voter['url'])\n",
    "            print('Find {} votes of voter id {}'.format(len(vote_soup), row_idx))\n",
    "        for v in vote_soup:\n",
    "            v_soup = v.find_all('p')\n",
    "            if len(v_soup) == 0:\n",
    "                print(voter['url'])\n",
    "                raise RuntimeError('Cannot find votes of voter ' + str(row_idx))\n",
    "            film_name = v.find_all('p')[0].get_text()\n",
    "            year_text = v.find_all('p')[1].get_text()\n",
    "            year = int(year_text) if year_text.isdigit() else None\n",
    "            director_name = v.find_all('p')[2].get_text()\n",
    "            # Find the film by matching film name, year and director\n",
    "            col_range = [i for i in range(len(films)) if films[i]['name'] == film_name and films[i]['year'] == year and films[i]['director'] == director_name]\n",
    "            # If the film cannot be identified, find the film with film name and year only\n",
    "            if len(col_range) == 0:\n",
    "                col_range = [i for i in range(len(films)) if films[i]['name'] == film_name and films[i]['year'] == year]\n",
    "            if len(col_range) != 1:\n",
    "                raise RuntimeError('Find {} films with name {}'.format(len(col_range), film_name))\n",
    "            col_idx = col_range[0]\n",
    "            votes_data.append([row_idx, col_idx])\n",
    "        comment = soup.find('div', {\"class\": \"wysiwyg\"})\n",
    "        if comment is None:\n",
    "            comments.append('')\n",
    "        else:\n",
    "            comments.append(comment.get_text())\n",
    "        # Save the result after every vote page visit\n",
    "        with open(VOTES_PATH, 'w') as outfile:\n",
    "            json.dump(votes_data, outfile)\n",
    "        with open(COMMENTS_PATH, 'w') as outfile:\n",
    "            json.dump(comments, outfile)\n",
    "        # Delay between parsing\n",
    "        print('Scraped votes of voter {}/{}'.format(row_idx + 1, len(voters)))\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(SCRAPE_DELAY)\n",
    "    print('Scraped all votes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory for data folder if it does not exist\n",
    "os.makedirs(FOLDER_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Films are parsed already\n",
      "Films are parsed already\n",
      "Found 2567 films\n",
      "Found 1205 voters\n",
      "Votes are saved in data/votes.json\n",
      "Comments are saved in data/comments.json\n",
      "Start scraping from voter id 1205\n",
      "Scraped all votes\n"
     ]
    }
   ],
   "source": [
    "scrape_films()\n",
    "scrape_voters()\n",
    "# Load films and voters data\n",
    "with open(FILMS_PATH) as json_file:\n",
    "    films = json.load(json_file)\n",
    "with open(VOTERS_PATH) as json_file:\n",
    "    voters = json.load(json_file)\n",
    "print('Found {} films'.format(len(films)))\n",
    "print('Found {} voters'.format(len(voters)))\n",
    "scrape_votes(films, voters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:snsl] *",
   "language": "python",
   "name": "conda-env-snsl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
